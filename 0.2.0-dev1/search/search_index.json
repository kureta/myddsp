{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MyDDSP Docs \u00b6 Reference \u00b6 MyDDSP \u00b6 48kHz, stereo DDSP based on Google Magenta's paper : You can train the model on a set of monophonic (single pitch) recordings. It also includes a real- time inference implementation. Constants module \u00b6 Constants for sample pre-processing. Preprocessors module \u00b6 Provides a few audio feature extractors. This module allows the user to extract audio features for feeding into the model. The module contains the following feature extractors: LegacyLoudness - The one I used originally. Its dynamic range is somehow compressed, and it is noisy. Loudness - Based on Loudness from torchaudio . center ( y , window_length = C . N_FFT , hop_length = C . HOP_LENGTH ) \u00b6 Centers signal based on fft window size. Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int window length C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description y_centered Tensor centered tensor of shape [..., S] Raises: Type Description ValueError window length must be divisible by 2 Source code in myddsp/preprocessors.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def center ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Centers signal based on fft window size. Args: y: tensor of shape `[..., S]` window_length: window length hop_length: hop length Returns: y_centered: centered tensor of shape `[..., S]` Raises: ValueError: window length must be divisible by 2 \"\"\" if ( window_length % 2 ) != 0 : ValueError ( \"Window length must be divisible by 2\" ) y_divisible = make_divisible_by_hop_length ( y , window_length , hop_length ) padding = window_length // 2 centered = F . pad ( y_divisible , ( padding , padding )) return centered get_centered_frames ( y , window_length = C . N_FFT , hop_length = C . HOP_LENGTH ) \u00b6 Generates centered frames of moving windows given hop length. Centeres the audio before calling get_frames on it. Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int length of each frame C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description frames Tensor audio frames of shape [..., W, F] Examples: >>> signal = torch . randn ( 8 , 2 , 48000 ) >>> signal_frames = get_centered_frames ( signal ) >>> signal_frames . shape torch.Size([8, 2, 3072, 251]) Source code in myddsp/preprocessors.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def get_centered_frames ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Generates centered frames of moving windows given hop length. Centeres the audio before calling `get_frames` on it. Args: y: tensor of shape `[..., S]` window_length: length of each frame hop_length: hop length Returns: frames: audio frames of shape `[..., W, F]` Examples: >>> signal = torch.randn(8, 2, 48000) >>> signal_frames = get_centered_frames(signal) >>> signal_frames.shape torch.Size([8, 2, 3072, 251]) \"\"\" centered = center ( y , window_length , hop_length ) frames = get_frames ( centered , window_length , hop_length ) return frames get_frames ( y , window_length = C . N_FFT , hop_length = C . HOP_LENGTH ) \u00b6 Generates frames of moving windows given hop length. Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int length of each frame C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description frames Tensor audio frames of shape [..., W, F] Examples: >>> signal = torch . randn ( 8 , 2 , 48000 ) >>> signal_frames = get_frames ( signal ) >>> signal_frames . shape torch.Size([8, 2, 3072, 235]) Raises: Type Description ValueError if tensor length is shorter than window length. Source code in myddsp/preprocessors.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def get_frames ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Generates frames of moving windows given hop length. Args: y: tensor of shape `[..., S]` window_length: length of each frame hop_length: hop length Returns: frames: audio frames of shape `[..., W, F]` Examples: >>> signal = torch.randn(8, 2, 48000) >>> signal_frames = get_frames(signal) >>> signal_frames.shape torch.Size([8, 2, 3072, 235]) Raises: ValueError: if tensor length is shorter than window length. \"\"\" y_divisible = make_divisible_by_hop_length ( y , window_length , hop_length ) length = y_divisible . shape [ - 1 ] if length < window_length : raise ValueError ( \"Example length cannot be shorter than the length of a single frame!\" ) frames = make_frames ( y_divisible , window_length , hop_length ) return frames make_divisible_by_hop_length ( y , window_length = C . N_FFT , hop_length = C . HOP_LENGTH ) \u00b6 Right pads a batch of audio examples to the nearest multiple of HOP_SIZE Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int window length C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description y_padded Tensor right padded examples, tensor of shape [..., S] Raises: Type Description ValueError if tensor is not in the required shape. Examples: >>> signal = torch . randn ( 1 , 2 , 48003 ) >>> signal_padded = make_divisible_by_hop_length ( signal ) >>> signal_padded . shape [ - 1 ] % C . HOP_LENGTH == 0 True >>> signal = torch . randn ( 1 , 2 , 0 ) >>> make_divisible_by_hop_length ( signal ) Traceback (most recent call last): ... ValueError:... Source code in myddsp/preprocessors.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def make_divisible_by_hop_length ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Right pads a batch of audio examples to the nearest multiple of `HOP_SIZE` Args: y: tensor of shape `[..., S]` window_length: window length hop_length: hop length Returns: y_padded: right padded examples, tensor of shape `[..., S]` Raises: ValueError: if tensor is not in the required shape. Examples: >>> signal = torch.randn(1, 2, 48003) >>> signal_padded = make_divisible_by_hop_length(signal) >>> signal_padded.shape[-1] % C.HOP_LENGTH == 0 True >>> signal = torch.randn(1, 2, 0) >>> make_divisible_by_hop_length(signal) Traceback (most recent call last): ... ValueError:... \"\"\" length = y . shape [ - 1 ] if length <= 0 : raise ValueError ( \"Example length must be strictly positive!\" ) remainder = ( length - window_length ) % hop_length if remainder == 0 : # already divisible by hop length return y padding = hop_length - remainder y_padded = F . pad ( y , ( 0 , padding ), mode = \"constant\" , value = 0.0 ) return y_padded","title":"Welcome to MyDDSP Docs"},{"location":"#welcome-to-myddsp-docs","text":"","title":"Welcome to MyDDSP Docs"},{"location":"#reference","text":"","title":"Reference"},{"location":"#myddsp_1","text":"48kHz, stereo DDSP based on Google Magenta's paper : You can train the model on a set of monophonic (single pitch) recordings. It also includes a real- time inference implementation.","title":"MyDDSP"},{"location":"#constants-module","text":"Constants for sample pre-processing.","title":"Constants module"},{"location":"#preprocessors-module","text":"Provides a few audio feature extractors. This module allows the user to extract audio features for feeding into the model. The module contains the following feature extractors: LegacyLoudness - The one I used originally. Its dynamic range is somehow compressed, and it is noisy. Loudness - Based on Loudness from torchaudio .","title":"Preprocessors module"},{"location":"#myddsp.preprocessors.center","text":"Centers signal based on fft window size. Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int window length C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description y_centered Tensor centered tensor of shape [..., S] Raises: Type Description ValueError window length must be divisible by 2 Source code in myddsp/preprocessors.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def center ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Centers signal based on fft window size. Args: y: tensor of shape `[..., S]` window_length: window length hop_length: hop length Returns: y_centered: centered tensor of shape `[..., S]` Raises: ValueError: window length must be divisible by 2 \"\"\" if ( window_length % 2 ) != 0 : ValueError ( \"Window length must be divisible by 2\" ) y_divisible = make_divisible_by_hop_length ( y , window_length , hop_length ) padding = window_length // 2 centered = F . pad ( y_divisible , ( padding , padding )) return centered","title":"center()"},{"location":"#myddsp.preprocessors.get_centered_frames","text":"Generates centered frames of moving windows given hop length. Centeres the audio before calling get_frames on it. Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int length of each frame C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description frames Tensor audio frames of shape [..., W, F] Examples: >>> signal = torch . randn ( 8 , 2 , 48000 ) >>> signal_frames = get_centered_frames ( signal ) >>> signal_frames . shape torch.Size([8, 2, 3072, 251]) Source code in myddsp/preprocessors.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def get_centered_frames ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Generates centered frames of moving windows given hop length. Centeres the audio before calling `get_frames` on it. Args: y: tensor of shape `[..., S]` window_length: length of each frame hop_length: hop length Returns: frames: audio frames of shape `[..., W, F]` Examples: >>> signal = torch.randn(8, 2, 48000) >>> signal_frames = get_centered_frames(signal) >>> signal_frames.shape torch.Size([8, 2, 3072, 251]) \"\"\" centered = center ( y , window_length , hop_length ) frames = get_frames ( centered , window_length , hop_length ) return frames","title":"get_centered_frames()"},{"location":"#myddsp.preprocessors.get_frames","text":"Generates frames of moving windows given hop length. Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int length of each frame C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description frames Tensor audio frames of shape [..., W, F] Examples: >>> signal = torch . randn ( 8 , 2 , 48000 ) >>> signal_frames = get_frames ( signal ) >>> signal_frames . shape torch.Size([8, 2, 3072, 235]) Raises: Type Description ValueError if tensor length is shorter than window length. Source code in myddsp/preprocessors.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def get_frames ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Generates frames of moving windows given hop length. Args: y: tensor of shape `[..., S]` window_length: length of each frame hop_length: hop length Returns: frames: audio frames of shape `[..., W, F]` Examples: >>> signal = torch.randn(8, 2, 48000) >>> signal_frames = get_frames(signal) >>> signal_frames.shape torch.Size([8, 2, 3072, 235]) Raises: ValueError: if tensor length is shorter than window length. \"\"\" y_divisible = make_divisible_by_hop_length ( y , window_length , hop_length ) length = y_divisible . shape [ - 1 ] if length < window_length : raise ValueError ( \"Example length cannot be shorter than the length of a single frame!\" ) frames = make_frames ( y_divisible , window_length , hop_length ) return frames","title":"get_frames()"},{"location":"#myddsp.preprocessors.make_divisible_by_hop_length","text":"Right pads a batch of audio examples to the nearest multiple of HOP_SIZE Parameters: Name Type Description Default y Tensor tensor of shape [..., S] required window_length int window length C.N_FFT hop_length int hop length C.HOP_LENGTH Returns: Name Type Description y_padded Tensor right padded examples, tensor of shape [..., S] Raises: Type Description ValueError if tensor is not in the required shape. Examples: >>> signal = torch . randn ( 1 , 2 , 48003 ) >>> signal_padded = make_divisible_by_hop_length ( signal ) >>> signal_padded . shape [ - 1 ] % C . HOP_LENGTH == 0 True >>> signal = torch . randn ( 1 , 2 , 0 ) >>> make_divisible_by_hop_length ( signal ) Traceback (most recent call last): ... ValueError:... Source code in myddsp/preprocessors.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def make_divisible_by_hop_length ( y : Tensor , window_length : int = C . N_FFT , hop_length : int = C . HOP_LENGTH ) -> Tensor : \"\"\"Right pads a batch of audio examples to the nearest multiple of `HOP_SIZE` Args: y: tensor of shape `[..., S]` window_length: window length hop_length: hop length Returns: y_padded: right padded examples, tensor of shape `[..., S]` Raises: ValueError: if tensor is not in the required shape. Examples: >>> signal = torch.randn(1, 2, 48003) >>> signal_padded = make_divisible_by_hop_length(signal) >>> signal_padded.shape[-1] % C.HOP_LENGTH == 0 True >>> signal = torch.randn(1, 2, 0) >>> make_divisible_by_hop_length(signal) Traceback (most recent call last): ... ValueError:... \"\"\" length = y . shape [ - 1 ] if length <= 0 : raise ValueError ( \"Example length must be strictly positive!\" ) remainder = ( length - window_length ) % hop_length if remainder == 0 : # already divisible by hop length return y padding = hop_length - remainder y_padded = F . pad ( y , ( 0 , padding ), mode = \"constant\" , value = 0.0 ) return y_padded","title":"make_divisible_by_hop_length()"}]}